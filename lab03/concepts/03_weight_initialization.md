# Concept: Weight Initialization

## ğŸ¯ Objective
Choose initial weights to maintain healthy signal/gradient magnitudes across layers.

---

## ğŸ“– Why initialization matters
- Poor scale â†’ activations saturate or explode
- Gradients vanish/explode â†’ training stalls or diverges
- Good initialization keeps variance stable layer-to-layer

---

## ğŸ“ Variance-preserving schemes
Let $n_{\text{in}}$ be fan-in (inputs to a layer) and $n_{\text{out}}$ be fan-out.

### Xavier/Glorot (tanh/sigmoid)
- Aim: keep activations/gradients variance constant
- Gaussian: $\mathcal{N}(0, \frac{2}{n_{\text{in}} + n_{\text{out}}})$
- Uniform: $\mathcal{U}\big[-\sqrt{\tfrac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\tfrac{6}{n_{\text{in}} + n_{\text{out}}}}\big]$

### He/Kaiming (ReLU)
- Accounts for ReLUâ€™s zeroing of negative inputs
- Gaussian: $\mathcal{N}(0, \frac{2}{n_{\text{in}}})$
- Uniform: $\mathcal{U}\big[-\sqrt{\tfrac{6}{n_{\text{in}}}}, \sqrt{\tfrac{6}{n_{\text{in}}}}\big]$

Biases: initialize to zeros or small constants.

---

## ğŸ§  Intuition and caveats
- Match scheme to activation: Xavier for tanh/sigmoid, He for ReLU
- BatchNorm can reduce sensitivity to initialization but doesnâ€™t remove it
- Very deep nets may still face gradient issues without additional tricks

---

## âœ… Success criteria
- You can pick Xavier vs He based on activation type
- You can state the common Gaussian/uniform forms and parameters
- You can explain the goal: stable variance through depth
