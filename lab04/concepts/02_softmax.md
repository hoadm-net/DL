# Concept: Softmax for Multi-class Classification

## ğŸ¯ Objective
Convert scores (logits) into probabilities over K classes while preserving relative differences.

---

## ğŸ“– Definition
Given logits $\mathbf{z} \in \mathbb{R}^K$:
$$\text{softmax}(\mathbf{z})_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$

Properties:
- Range: $(0,1)$ and sums to 1
- Order preserving: increasing one logit increases its probability
- Translation invariant: adding a constant to all logits doesnâ€™t change probs

---

## ğŸ§  Intuition
- Exponentiation magnifies differences â†’ sharper distributions
- Softmax turns scores into a categorical distribution over classes

---

## âš ï¸ Stability: subtract-max trick
Compute with $\tilde{z}_k = z_k - \max_j z_j$:
$$\text{softmax}(\mathbf{z})_k = \frac{e^{\tilde{z}_k}}{\sum_j e^{\tilde{z}_j}}$$
This prevents overflow when logits are large.

---

## âœ… Success criteria
- You can define softmax and list its key properties
- You can explain the subtract-max trick and why itâ€™s safe
- You can interpret softmax outputs as probabilities
