# Deep Learning from Scratch - Lab-based Learning

Hands-on Deep Learning course with Python + Numpy, from basic concepts to practical implementation.

## ðŸŽ¯ Objectives
- Understand deeply how Neural Networks work
- Code from scratch without frameworks (TensorFlow, PyTorch)
- Master the mathematics behind Deep Learning
- Learn through systematic hands-on labs

## ðŸ“š Lab Structure

### **Lab 01: Fundamental Concepts** ðŸ”¥
- Forward Propagation
- Loss Functions (MSE)
- Backward Propagation
- Gradient Descent

---

### **Lab 02: Activation Functions**
- Sigmoid, ReLU, Tanh
- Non-linearity in Neural Networks
- Vanishing gradient problem
- See: `lab02/`

### **Lab 03: Multi-layer Networks** (Coming Soon)
- Stacking layers
- Deep neural networks
- Backpropagation through multiple layers
 - See: `lab03/`

### **Lab 04: Classification Problems** (Coming Soon)
- Binary classification
- Softmax and Cross-entropy
- Multi-class classification
 - See: `lab04/`

### **Lab 05: Optimization Algorithms** (Coming Soon)
- SGD variations
- Momentum, Adam, RMSprop
- Learning rate scheduling
 - See: `lab05/`

### **Lab 06: Regularization** (Coming Soon)
- Overfitting problem
- L1/L2 regularization
- Dropout

### **Lab 07: Real Datasets** (Coming Soon)
- MNIST handwritten digits
- Data preprocessing
- Model evaluation

---

## ðŸš€ Getting Started

```bash
cd lab01/
# Start with the first lab
```