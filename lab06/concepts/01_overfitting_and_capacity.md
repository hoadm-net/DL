# Concept: Overfitting, Capacity, and Biasâ€“Variance

## ðŸŽ¯ Objective
Diagnose over/underfitting and understand capacity control through the biasâ€“variance lens.

---

## ðŸ“– Core ideas
- Overfitting: low training error, high validation error
- Underfitting: high training and validation error
- Capacity: modelâ€™s ability to fit complex patterns (depth, width, parameters)
- Biasâ€“variance trade-off: increasing capacity â†“ bias, â†‘ variance

---

## ðŸ§  Intuition
- Too simple â†’ canâ€™t fit data (high bias)
- Too complex â†’ memorizes noise (high variance)
- Sweet spot â†’ minimal validation error

Learning curves (error vs. training size/epochs) help diagnose which regime youâ€™re in.

---

## ðŸ§° Tools
- Cross-validation and a proper validation split
- Data augmentation (images, text): enrich diversity without new labels
- Early stopping: halt when validation worsens

---

## âœ… Success criteria
- You can identify over/underfitting from training vs validation curves
- You can propose capacity adjustments and data-centric fixes
- You can relate biasâ€“variance to regularization choices
